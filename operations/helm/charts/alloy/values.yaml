# -- Overrides the chart's name. Used to change the infix in the resource names.
nameOverride: null

# -- Overrides the chart's namespace.
namespaceOverride: null

# -- Overrides the chart's computed fullname. Used to change the full prefix of
# resource names.
fullnameOverride: null

## Global properties for image pulling override the values defined under `image.registry` and `configReloader.image.registry`.
## If you want to override only one image registry, use the specific fields but if you want to override them all, use `global.image.registry`
global:
  image:
    # -- Global image registry to use if it needs to be overridden for some specific use cases (e.g local registries, custom images, ...)
    registry: ""

    # -- Optional set of global image pull secrets.
    pullSecrets: []

  # -- Security context to apply to the Grafana Alloy pod.
  podSecurityContext: {}

crds:
  # -- Whether to install CRDs for monitoring.
  create: true

## Various Alloy settings. For backwards compatibility with the grafana-agent
## chart, this field may also be called "agent". Naming this field "agent" is
## deprecated and will be removed in a future release.
alloy:
  configMap:
    # -- Create a new ConfigMap for the config file.
    create: true
    # -- Content to assign to the new ConfigMap.  This is passed into `tpl` allowing for templating from values.
    content: ''

    # -- Name of existing ConfigMap to use. Used when create is false.
    name: null
    # -- Key in ConfigMap to get config from.
    key: null

  clustering:
    # -- Deploy Alloy in a cluster to allow for load distribution.
    enabled: false

    # -- Name for the Alloy cluster. Used for differentiating between clusters.
    name: ""

    # -- Name for the port used for clustering, useful if running inside an Istio Mesh
    portName: http

  # -- Minimum stability level of components and behavior to enable. Must be
  # one of "experimental", "public-preview", or "generally-available".
  stabilityLevel: "generally-available"

  # -- Path to where Grafana Alloy stores data (for example, the Write-Ahead Log).
  # By default, data is lost between reboots.
  storagePath: /tmp/alloy

  # -- Enables Grafana Alloy container's http server port.
  enableHttpServerPort: true

  # -- Address to listen for traffic on. 0.0.0.0 exposes the UI to other
  # containers.
  listenAddr: 0.0.0.0

  # -- Port to listen for traffic on.
  listenPort: 12345

  # -- Scheme is needed for readiness probes. If enabling tls in your configs, set to "HTTPS"
  listenScheme: HTTP

  # -- Initial delay for readiness probe.
  initialDelaySeconds: 10

  # -- Timeout for readiness probe.
  timeoutSeconds: 1

  # --  Base path where the UI is exposed.
  uiPathPrefix: /

  # -- Enables sending Grafana Labs anonymous usage stats to help improve Grafana
  # Alloy.
  enableReporting: true

  # -- Extra environment variables to pass to the Alloy container.
  extraEnv: []

  # -- Maps all the keys on a ConfigMap or Secret as environment variables. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#envfromsource-v1-core
  envFrom: []

  # -- Extra args to pass to `alloy run`: https://grafana.com/docs/alloy/latest/reference/cli/run/
  extraArgs: []

  # -- Extra ports to expose on the Alloy container.
  extraPorts: []
  # - name: "faro"
  #   port: 12347
  #   targetPort: 12347
  #   protocol: "TCP"
  #   appProtocol: "h2c"

  # -- Host aliases to add to the Alloy container.
  hostAliases: []
  # - ip: "20.21.22.23"
  #   hostnames:
  #     - "company.grafana.net"

  mounts:
    # -- Mount /var/log from the host into the container for log collection.
    varlog: false
    # -- Mount /var/lib/docker/containers from the host into the container for log
    # collection.
    dockercontainers: false

    # -- Extra volume mounts to add into the Grafana Alloy container. Does not
    # affect the watch container.
    extra: []

  # -- Security context to apply to the Grafana Alloy container.
  securityContext: {}

  # -- Resource requests and limits to apply to the Grafana Alloy container.
  resources: {}

  # -- Set lifecycle hooks for the Grafana Alloy container.
  lifecycle: {}
    # preStop:
    #   exec:
    #     command:
    #     - /bin/sleep
    #     - "10"

  # -- Set livenessProbe for the Grafana Alloy container.
  livenessProbe: {}

image:
  # -- Grafana Alloy image registry (defaults to docker.io)
  registry: "docker.io"
  # -- Grafana Alloy image repository.
  repository: grafana/alloy
  # -- (string) Grafana Alloy image tag. When empty, the Chart's appVersion is
  # used.
  tag: null
  # -- Grafana Alloy image's SHA256 digest (either in format "sha256:XYZ" or "XYZ"). When set, will override `image.tag`.
  digest: null
  # -- Grafana Alloy image pull policy.
  pullPolicy: IfNotPresent
  # -- Optional set of image pull secrets.
  pullSecrets: []

rbac:
  # -- Whether to create RBAC resources for Alloy.
  create: true

  # -- If set, only create Roles and RoleBindings in the given list of namespaces, rather than ClusterRoles and
  # ClusterRoleBindings. If not using ClusterRoles, bear in mind that Alloy will not be able to discover cluster-scoped
  # resources such as Nodes.
  namespaces: []

  # -- The rules to create for the ClusterRole or Role objects.
  rules:
    # -- Rules required for the `discovery.kubernetes` component.
    - apiGroups: ["", "discovery.k8s.io", "networking.k8s.io"]
      resources: ["endpoints", "endpointslices", "ingresses", "pods", "services"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `loki.source.kubernetes` component.
    - apiGroups: [""]
      resources: ["pods", "pods/log", "namespaces"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `loki.source.podlogs` component.
    - apiGroups: ["monitoring.grafana.com"]
      resources: ["podlogs"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `mimir.rules.kubernetes` component.
    - apiGroups: ["monitoring.coreos.com"]
      resources: ["prometheusrules"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `mimir.alerts.kubernetes` component.
    - apiGroups: ["monitoring.coreos.com"]
      resources: ["alertmanagerconfigs"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `prometheus.operator.*` components.
    - apiGroups: ["monitoring.coreos.com"]
      resources: ["podmonitors", "servicemonitors", "probes", "scrapeconfigs"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `loki.source.kubernetes_events` component.
    - apiGroups: [""]
      resources: ["events"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `remote.kubernetes.*` components.
    - apiGroups: [""]
      resources: ["configmaps", "secrets"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `otelcol.processor.k8sattributes` component.
    - apiGroups: ["apps", "extensions"]
      resources: ["replicasets"]
      verbs: ["get", "list", "watch"]

  # -- The rules to create for the ClusterRole objects.
  clusterRules:
    # -- Rules required for the Nodes role in the `discovery.kubernetes` component.
    - apiGroups: [""]
      resources: ["nodes"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for the `discovery.kubelet` component.
    - apiGroups: [""]
      resources: ["nodes/pods"]
      verbs: ["get", "list", "watch"]
    # -- Rules required accessing metric endpoints on the Node (e.g. Kubelet, cAdvisor, etc...).
    - apiGroups: [""]
      resources: ["nodes/metrics"]
      verbs: ["get", "list", "watch"]
    # -- Rules required for accessing metrics endpoint.
    - nonResourceURLs: ["/metrics"]
      verbs: ["get"]

serviceAccount:
  # -- Whether to create a service account for the Grafana Alloy deployment.
  create: true
  # -- Additional labels to add to the created service account.
  additionalLabels: {}
  # -- Annotations to add to the created service account.
  annotations: {}
  # -- The name of the existing service account to use when
  # serviceAccount.create is false.
  name: null
  # Whether the Alloy pod should automatically mount the service account token.
  automountServiceAccountToken: true

# Options for the extra controller used for config reloading.
configReloader:
  # -- Enables automatically reloading when the Alloy config changes.
  enabled: true
  image:
    # -- Config reloader image registry (defaults to docker.io)
    registry: "quay.io"
    # -- Repository to get config reloader image from.
    repository: prometheus-operator/prometheus-config-reloader
    # -- Tag of image to use for config reloading.
    tag: v0.81.0
    # -- SHA256 digest of image to use for config reloading (either in format "sha256:XYZ" or "XYZ"). When set, will override `configReloader.image.tag`
    digest: ""
  # -- Override the args passed to the container.
  customArgs: []
  # -- Resource requests and limits to apply to the config reloader container.
  resources:
    requests:
      cpu: "10m"
      memory: "50Mi"
  # -- Security context to apply to the Grafana configReloader container.
  securityContext: {}

controller:
  # -- Type of controller to use for deploying Grafana Alloy in the cluster.
  # Must be one of 'daemonset', 'deployment', or 'statefulset'.
  type: 'daemonset'

  # -- Number of pods to deploy. Ignored when controller.type is 'daemonset'.
  replicas: 1

  # -- Extra labels to add to the controller.
  extraLabels: {}

  # -- Annotations to add to controller.
  extraAnnotations: {}

  # -- Whether to deploy pods in parallel. Only used when controller.type is
  # 'statefulset'.
  parallelRollout: true

  # -- How many additional seconds to wait before considering a pod ready.
  minReadySeconds: 10

  # -- Configures Pods to use the host network. When set to true, the ports that will be used must be specified.
  hostNetwork: false

  # -- Configures Pods to use the host PID namespace.
  hostPID: false

  # -- Configures the DNS policy for the pod. https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: ClusterFirst

  # -- Termination grace period in seconds for the Grafana Alloy pods.
  # The default value used by Kubernetes if unspecifed is 30 seconds.
  terminationGracePeriodSeconds: null

  # -- Update strategy for updating deployed Pods.
  updateStrategy: {}

  # -- nodeSelector to apply to Grafana Alloy pods.
  nodeSelector: {}

  # -- Tolerations to apply to Grafana Alloy pods.
  tolerations: []

  # -- Topology Spread Constraints to apply to Grafana Alloy pods.
  topologySpreadConstraints: []

  # -- priorityClassName to apply to Grafana Alloy pods.
  priorityClassName: ''

  # -- Extra pod annotations to add.
  podAnnotations: {}

  # -- Extra pod labels to add.
  podLabels: {}

  # -- PodDisruptionBudget configuration.
  podDisruptionBudget:
    # -- Whether to create a PodDisruptionBudget for the controller.
    enabled: false
    # -- Minimum number of pods that must be available during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    minAvailable: null
    # -- Maximum number of pods that can be unavailable during a disruption.
    # Note: Only one of minAvailable or maxUnavailable should be set.
    maxUnavailable: null

  # -- Whether to enable automatic deletion of stale PVCs due to a scale down operation, when controller.type is 'statefulset'.
  enableStatefulSetAutoDeletePVC: false

  autoscaling:
    # -- Creates a HorizontalPodAutoscaler for controller type deployment.
    # Deprecated: Please use controller.autoscaling.horizontal instead
    enabled: false
    # -- The lower limit for the number of replicas to which the autoscaler can scale down.
    minReplicas: 1
    # -- The upper limit for the number of replicas to which the autoscaler can scale up.
    maxReplicas: 5
    # -- Average CPU utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetCPUUtilizationPercentage` to 0 will disable CPU scaling.
    targetCPUUtilizationPercentage: 0
    # -- Average Memory utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetMemoryUtilizationPercentage` to 0 will disable Memory scaling.
    targetMemoryUtilizationPercentage: 80

    scaleDown:
      # -- List of policies to determine the scale-down behavior.
      policies: []
        # - type: Pods
        #   value: 4
        #   periodSeconds: 60
      # -- Determines which of the provided scaling-down policies to apply if multiple are specified.
      selectPolicy: Max
      # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling down.
      stabilizationWindowSeconds: 300

    scaleUp:
      # -- List of policies to determine the scale-up behavior.
      policies: []
        # - type: Pods
        #   value: 4
        #   periodSeconds: 60
      # -- Determines which of the provided scaling-up policies to apply if multiple are specified.
      selectPolicy: Max
      # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling up.
      stabilizationWindowSeconds: 0

    # -- Configures the Horizontal Pod Autoscaler for the controller.
    horizontal:
      # -- Enables the Horizontal Pod Autoscaler for the controller.
      enabled: false

      # -- The lower limit for the number of replicas to which the autoscaler can scale down.
      minReplicas: 1
      # -- The upper limit for the number of replicas to which the autoscaler can scale up.
      maxReplicas: 5
      # -- Average CPU utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetCPUUtilizationPercentage` to 0 will disable CPU scaling.
      targetCPUUtilizationPercentage: 0
      # -- Average Memory utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetMemoryUtilizationPercentage` to 0 will disable Memory scaling.
      targetMemoryUtilizationPercentage: 80

      scaleDown:
        # -- List of policies to determine the scale-down behavior.
        policies: []
          # - type: Pods
          #   value: 4
          #   periodSeconds: 60
        # -- Determines which of the provided scaling-down policies to apply if multiple are specified.
        selectPolicy: Max
        # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling down.
        stabilizationWindowSeconds: 300

      scaleUp:
        # -- List of policies to determine the scale-up behavior.
        policies: []
          # - type: Pods
          #   value: 4
          #   periodSeconds: 60
        # -- Determines which of the provided scaling-up policies to apply if multiple are specified.
        selectPolicy: Max
        # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling up.
        stabilizationWindowSeconds: 0
    # -- Configures the Vertical Pod Autoscaler for the controller.
    vertical:
      # -- Enables the Vertical Pod Autoscaler for the controller.
      enabled: false

      # -- List of recommenders to use for the Vertical Pod Autoscaler.
      # Recommenders are responsible for generating recommendation for the object.
      # List should be empty (then the default recommender will generate the recommendation)
      # or contain exactly one recommender.
      recommenders: []
      # recommenders:
      # - name: custom-recommender-performance

      # -- Configures the resource policy for the Vertical Pod Autoscaler.
      resourcePolicy:
        # -- Configures the container policies for the Vertical Pod Autoscaler.
        containerPolicies:
        - containerName: alloy
          # -- The controlled resources for the Vertical Pod Autoscaler.
          controlledResources:
          - cpu
          - memory
          # -- The controlled values for the Vertical Pod Autoscaler. Needs to be either RequestsOnly or RequestsAndLimits.
          controlledValues: "RequestsAndLimits"
          # -- The maximum allowed values for the pods.
          maxAllowed: {}
          # cpu: 200m
          # memory: 100Mi
          # -- Defines the min allowed resources for the pod
          minAllowed: {}
          # cpu: 200m
          # memory: 100Mi

      # -- Configures the update policy for the Vertical Pod Autoscaler.
      updatePolicy:
        # -- Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
        # minReplicas: 1
        # -- Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
        # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
        # updateMode: Auto

  # -- Affinity configuration for pods.
  affinity: {}

  volumes:
    # -- Extra volumes to add to the Grafana Alloy pod.
    extra: []

  # -- volumeClaimTemplates to add when controller.type is 'statefulset'.
  volumeClaimTemplates: []

  ## -- Additional init containers to run.
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  ##
  initContainers: []

  # -- Additional containers to run alongside the Alloy container and initContainers.
  extraContainers: []

networkPolicy:
  enabled: false
  flavor: kubernetes

  policyTypes:
    - Ingress
    - Egress

  # Default allow all traffic because Alloy is so configurable
  # It is recommended to change this before deploying to production
  # To disable each policyType, set value to `null`
  ingress:
    - {}
  egress:
    - {}

service:
  # -- Creates a Service for the controller's pods.
  enabled: true
  # -- Service type
  type: ClusterIP
  # -- NodePort port. Only takes effect when `service.type: NodePort`
  nodePort: 31128
  # -- Cluster IP, can be set to None, empty "" or an IP address
  clusterIP: ''
  # -- Value for internal traffic policy. 'Cluster' or 'Local'
  internalTrafficPolicy: Cluster
  annotations: {}
    # cloud.google.com/load-balancer-type: Internal

serviceMonitor:
  enabled: false
  # -- Additional labels for the service monitor.
  additionalLabels: {}
  # -- Scrape interval. If not set, the Prometheus default scrape interval is used.
  interval: ""
  # -- MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
  metricRelabelings: []
  # - action: keep
  #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
  #   sourceLabels: [__name__]

  # -- Customize tls parameters for the service monitor
  tlsConfig: {}

  # -- RelabelConfigs to apply to samples before scraping
  # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
  relabelings: []
  # - sourceLabels: [__meta_kubernetes_pod_node_name]
  #   separator: ;
  #   regex: ^(.*)$
  #   targetLabel: nodename
  #   replacement: $1
  #   action: replace
ingress:
  # -- Enables ingress for Alloy (Faro port).
  # When gateway.enabled is true, ingress will automatically route to the gateway service instead.
  enabled: false
  # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  # ingressClassName: nginx
  # Values can be templated
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  labels: {}
  path: /
  # -- Port to use for ingress. Automatically uses gateway port when gateway.enabled is true.
  faroPort: 12347

  # pathType is only for k8s >= 1.1=
  pathType: Prefix

  hosts:
    - chart-example.local
  ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
  extraPaths: []
  # - path: /*
  #   backend:
  #     serviceName: ssl-redirect
  #     servicePort: use-annotation
  ## Or for k8s > 1.19
  # - path: /*
  #   pathType: Prefix
  #   backend:
  #     service:
  #       name: ssl-redirect
  #       port:
  #         name: use-annotation

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# -- Extra k8s manifests to deploy
extraObjects: []
# - apiVersion: v1
#   kind: Secret
#   metadata:
#     name: grafana-cloud
#   stringData:
#     PROMETHEUS_HOST: 'https://prometheus-us-central1.grafana.net/api/prom/push'
#     PROMETHEUS_USERNAME: '123456'

gateway:
  # -- Enables the NGINX gateway deployment for unified HTTP(S) entry point.
  enabled: false

  # -- Number of gateway replicas to deploy.
  replicas: 2
  revisionHistoryLimit: null
  dnsConfig: {}
  # -- Annotations to add to the gateway deployment
  annotations: {}
  # -- Labels to add to the the gateway deployment
  labels: {}

  image:
    # -- NGINX image registry.
    registry: "docker.io"
    # -- NGINX image repository.
    repository: nginx
    # -- NGINX image tag.
    tag: "1.27-alpine"
    # -- NGINX image pull policy.
    pullPolicy: IfNotPresent
    # -- Optional set of image pull secrets for NGINX.
    pullSecrets: []

  # -- Port for the gateway service to listen on.
  port: 8080

  # -- Service type for the gateway.
  service:
    # -- Service type (ClusterIP, LoadBalancer, NodePort).
    type: ClusterIP
    # -- Annotations for the gateway service.
    annotations: {}
    # -- Labels for the gateway service.
    labels: {}
    # -- NodePort port. Only takes effect when `gateway.service.type: NodePort`.
    nodePort: null
    # -- LoadBalancer IP. Only takes effect when `gateway.service.type: LoadBalancer`.
    loadBalancerIP: null
    # -- LoadBalancer source ranges. Only takes effect when `gateway.service.type: LoadBalancer`.
    loadBalancerSourceRanges: []

  # -- Authentication configuration for the gateway.
  auth:
    # -- Enables basic authentication at the gateway level.
    enabled: false
    # -- Name of existing secret containing .htpasswd file.
    # The secret must contain a key named 'auth' with htpasswd-formatted content.
    # Create with: htpasswd -c .htpasswd user
    existingSecret: null
    # -- Realm for basic authentication.
    realm: "Alloy Gateway"

  # -- Protocol upstreams configuration.
  # Configure which protocols to enable and their routing.
  # Each key becomes an upstream name, and you can add custom upstreams.
  upstreams:
    # -- OTLP gRPC endpoint configuration.
    otlpGrpc:
      # -- Enable OTLP gRPC endpoint.
      enabled: true
      # -- Path for OTLP gRPC endpoint.
      path: /opentelemetry
      # -- Target port on Alloy service for OTLP gRPC.
      targetPort: 4317
      # -- Protocol type (http or grpc).
      protocol: grpc
      # -- Additional NGINX location block configuration for this upstream.
      extraConfig: ""

    # -- OTLP HTTP endpoint configuration.
    otlpHttp:
      # -- Enable OTLP HTTP endpoint.
      enabled: true
      # -- Path for OTLP HTTP endpoint.
      path: /otlp/v1
      # -- Target port on Alloy service for OTLP HTTP.
      targetPort: 4318
      # -- Protocol type (http or grpc).
      protocol: http
      # -- Additional NGINX location block configuration for this upstream.
      extraConfig: ""

    # -- Loki endpoint configuration.
    loki:
      # -- Enable Loki endpoint.
      enabled: true
      # -- Path for Loki endpoint.
      path: /loki/api/v1/push
      # -- Target port on Alloy service for Loki.
      targetPort: 3100
      # -- Protocol type (http or grpc).
      protocol: http
      # -- Additional NGINX location block configuration for this upstream.
      extraConfig: ""

    # -- Prometheus remote write endpoint configuration.
    prometheus:
      # -- Enable Prometheus endpoint.
      enabled: true
      # -- Path for Prometheus remote write endpoint.
      path: /api/v1/metrics/write
      # -- Target port on Alloy service for Prometheus.
      targetPort: 9009
      # -- Protocol type (http or grpc).
      protocol: http
      # -- Additional NGINX location block configuration for this upstream.
      extraConfig: ""

    # -- Pyroscope endpoint configuration.
    pyroscope:
      # -- Enable Pyroscope endpoint.
      enabled: false
      # -- Path for Pyroscope endpoint.
      path: /push.v1.PusherService/Push
      # -- Target port on Alloy service for Pyroscope.
      targetPort: 4100
      # -- Protocol type (http or grpc).
      protocol: http
      # -- Additional NGINX location block configuration for this upstream.
      extraConfig: ""

  # -- NGINX configuration settings.
  nginxConfig:
    # -- Worker processes for NGINX.
    workerProcesses: auto
    # -- Worker connections for NGINX.
    workerConnections: 1024
    # -- Client body buffer size.
    clientBodyBufferSize: "128k"
    # -- Client max body size.
    clientMaxBodySize: "10m"
    # -- Client header timeout.
    clientHeaderTimeout: "10s"
    # -- Proxy connect timeout.
    proxyConnectTimeout: "60s"
    # -- Proxy send timeout.
    proxySendTimeout: "60s"
    # -- Proxy read timeout.
    proxyReadTimeout: "60s"
    # -- Send timeout.
    sendTimeout: "60s"
    # -- Keepalive timeout.
    keepaliveTimeout: "65s"
    # -- Enable access logging. Disable for better performance in high-traffic environments.
    enableAccessLog: true
    # -- Custom log format for access logs.
    logFormat: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"'
    # -- Additional NGINX http block configuration.
    extraHttpConfig: ""
    # -- Additional NGINX server block configuration.
    extraServerConfig: ""
    # -- NGINX configuration file content.
    # This is passed through 'tpl' allowing Helm variables inside.
    file: |
      pid /tmp/nginx.pid; # Required for readOnlyRootFilesystem
      worker_processes {{ .Values.gateway.nginxConfig.workerProcesses | default "auto" }};
      error_log /tmp/error.log warn;
      
      events {
        worker_connections {{ .Values.gateway.nginxConfig.workerConnections | default 1024 }};
      }
      
      http {
        include /etc/nginx/mime.types;
        default_type application/octet-stream;
      
        log_format main '{{ .Values.gateway.nginxConfig.logFormat }}';
      
        {{- if .Values.gateway.nginxConfig.enableAccessLog }}
        map $status $loggable {
          ~^[23]  0;
          default 1;
        }
        log_format main '$remote_addr - $remote_user [$time_local]  $status '
          '"$request" $body_bytes_sent "$http_referer" '
          '"$http_user_agent" "$http_x_forwarded_for"';
        access_log /var/log/nginx/access.log main if=loggable;
        {{- else }}
        access_log off;
        {{- end }}
      
        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
      
        keepalive_timeout {{ .Values.gateway.nginxConfig.keepaliveTimeout }};
        send_timeout {{ .Values.gateway.nginxConfig.sendTimeout }};
      
        client_body_buffer_size {{ .Values.gateway.nginxConfig.clientBodyBufferSize }};
        client_max_body_size {{ .Values.gateway.nginxConfig.clientMaxBodySize | default "50m" }};
      
        client_header_timeout {{ .Values.gateway.nginxConfig.clientHeaderTimeout}};
        proxy_connect_timeout {{ .Values.gateway.nginxConfig.proxyConnectTimeout }};
        proxy_send_timeout {{ .Values.gateway.nginxConfig.proxySendTimeout }};
        proxy_read_timeout {{ .Values.gateway.nginxConfig.proxyReadTimeout }};
      
        proxy_buffering off;
        proxy_request_buffering off;
      
        # Upstream definitions
        {{- range $name, $upstream := .Values.gateway.upstreams }}
        {{- if $upstream.enabled }}
        upstream {{ include "alloy.fullname" $ }}-{{ $name }} {
          server {{ include "alloy.fullname" $ }}.{{ include "alloy.namespace" $ }}.svc.cluster.local:{{ $upstream.targetPort }};
          keepalive 32;
        }
        {{- end }}
        {{- end }}
      
        {{- if .Values.gateway.nginxConfig.extraHttpConfig }}
        {{ .Values.gateway.nginxConfig.extraHttpConfig | nindent 2 }}
        {{- end }}
      
        server {
          listen {{ .Values.gateway.port }} http2;
          {{- if .Values.gateway.tls.enabled }}
          listen {{ .Values.gateway.tls.port }} ssl;
          ssl_certificate /etc/nginx/tls/tls.crt;
          ssl_certificate_key /etc/nginx/tls/tls.key;
          ssl_protocols TLSv1.2 TLSv1.3;
          ssl_ciphers HIGH:!aNULL:!MD5;
          ssl_prefer_server_ciphers on;
          {{- end }}
      
          server_name _;
      
          # Health check endpoint
          location /health {
            access_log off;
            return 200 "OK";
            add_header Content-Type text/plain;
          }
      
          # Stub status for monitoring
          location /stub_status {
            stub_status on;
            access_log off;
            allow 127.0.0.1;
            deny all;
          }
      
          {{- if .Values.gateway.auth.enabled }}
          auth_basic "{{ .Values.gateway.auth.realm }}";
          auth_basic_user_file /etc/nginx/auth/.htpasswd;
          {{- end }}
      
          {{- range $name, $upstream := .Values.gateway.upstreams }}
          {{- if $upstream.enabled }}
          # {{ $name }} endpoint
          location ~ {{ $upstream.path }} {
            {{- if $upstream.extraConfig }}
            {{ $upstream.extraConfig | nindent 6 }}
            {{- else }}
            {{- if eq $upstream.protocol "grpc" }}
            grpc_pass grpc://{{ include "alloy.fullname" $ }}-{{ $name }};
            grpc_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            grpc_set_header X-Real-IP $remote_addr;
            {{- else }}
            proxy_pass http://{{ include "alloy.fullname" $ }}-{{ $name }}$request_uri;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Real-IP $remote_addr;
            {{- end }}
            {{- end }}
          }
          {{- end }}
          {{- end }}
      
          {{- if .Values.gateway.nginxConfig.extraServerConfig }}
          {{ .Values.gateway.nginxConfig.extraServerConfig | nindent 4 }}
          {{- end }}
        }
      }


  # -- TLS configuration for the gateway.
  tls:
    # -- Enable TLS.
    enabled: false
    # -- Name of secret containing TLS certificate and key.
    # The secret must contain 'tls.crt' and 'tls.key' keys.
    secretName: null
    # -- TLS port.
    port: 8443

  # -- Grace period to allow the gateway container to shut down before it is killed
  terminationGracePeriodSeconds: 30

  # -- Resource requests and limits for the gateway.
  resources: {}
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
  # limits:
  #   cpu: 200m
  #   memory: 256Mi

  # -- nodeSelector for gateway pods.
  nodeSelector: {}

  # -- Tolerations for gateway pods.
  tolerations: []

  # -- Affinity configuration for gateway pods.
  affinity: {}

  # -- Pod annotations for the gateway.
  podAnnotations: {}

  # -- Pod labels for the gateway.
  podLabels: {}

  # -- Security context for gateway pods.
  podSecurityContext:
    fsGroup: 101
    runAsGroup: 101
    runAsNonRoot: true
    runAsUser: 101

  # -- Security context for gateway container.
  securityContext: {}

  # -- The SecurityContext for gateway containers
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ ALL ]

  # -- Priority class name for gateway pods.
  priorityClassName: ""

  # -- Update strategy for the gateway deployment.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 15%

  # -- PodDisruptionBudget configuration for the gateway.
  podDisruptionBudget:
    # -- Whether to create a PodDisruptionBudget.
    enabled: false
    # -- Minimum number of pods that must be available.
    minAvailable: 1
    # -- Maximum number of pods that can be unavailable.
    maxUnavailable: null

  # -- HorizontalPodAutoscaler configuration for the gateway.
  autoscaling:
    # -- Enable HorizontalPodAutoscaler.
    enabled: false
    # -- Minimum number of replicas.
    minReplicas: 1
    # -- Maximum number of replicas.
    maxReplicas: 3
    # -- Target CPU utilization percentage.
    targetCPUUtilizationPercentage: 70
    # -- Target memory utilization percentage.
    targetMemoryUtilizationPercentage: 70

  # -- ServiceMonitor configuration for the gateway.
  serviceMonitor:
    # -- Enable ServiceMonitor for the gateway.
    enabled: false
    # -- Additional labels for the service monitor.
    additionalLabels: {}
    # -- Scrape interval.
    interval: ""
    # -- Scrape path for NGINX metrics (requires nginx-prometheus-exporter sidecar).
    path: /metrics
    # -- Scrape port for NGINX metrics.
    port: metrics

  # -- Extra environment variables for the gateway container.
  extraEnv: []

  # -- Extra volumes for the gateway pods.
  extraVolumes:
    - name: tmp
      emptyDir: {}
    - name: varcache
      emptyDir: {}

  # -- Extra volume mounts for the gateway container.
  extraVolumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: varcache
      mountPath: /var/cache/nginx

  # -- Extra containers to run in the gateway pod (e.g., nginx-prometheus-exporter).
  extraContainers: []
  # - name: nginx-exporter
  #   image: nginx/nginx-prometheus-exporter:1.1.0
  #   args:
  #     - -nginx.scrape-uri=http://localhost:8080/stub_status
  #   ports:
  #   - name: metrics
  #     containerPort: 9113
